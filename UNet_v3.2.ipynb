{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a87c049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/H-arshit/UNET-On-COCO/blob/master/tf_Keras_COCO_UNET.ipynb\n",
    "# Importing Data From COCO\n",
    "\n",
    "from pycocotools import coco, cocoeval, _mask\n",
    "from pycocotools import mask as maskUtils \n",
    "import array\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import os\n",
    "import shutil\n",
    "pylab.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9e37c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_DIR = 'coco-2017'\n",
    "TRAIN_IMG = COCO_DIR + '/train/data'\n",
    "VAL_IMG = COCO_DIR + '/validation/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e3adcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY_NAMES=['person']\n",
    "\n",
    "ANNOTATION_FILE_VAL = 'coco-2017/raw/instances_val2017.json'\n",
    "ANNOTATION_FILE_TRAIN = 'coco-2017/raw/instances_train2017.json'\n",
    "\n",
    "\n",
    "coco_train = coco.COCO(ANNOTATION_FILE_TRAIN)\n",
    "catIds_train = coco_train.getCatIds(catNms=CATEGORY_NAMES);\n",
    "imgIds_train = coco_train.getImgIds(catIds=catIds_train);\n",
    "imgDict_train = coco_train.loadImgs(imgIds_train)\n",
    "len(imgIds_train) , len(catIds_train)\n",
    "\n",
    "\n",
    "coco_val = coco.COCO(ANNOTATION_FILE_VAL)\n",
    "catIds_val = coco_val.getCatIds(catNms=CATEGORY_NAMES);\n",
    "imgIds_val = coco_val.getImgIds(catIds=catIds_val);\n",
    "imgDict_val = coco_val.loadImgs(imgIds_val)\n",
    "len(imgIds_val) , len(catIds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a465c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "shuffle(imgIds_train)\n",
    "shuffle(imgIds_val)\n",
    "\n",
    "imgIds_train = imgIds_train[0:6000]\n",
    "imgIds_val = imgIds_val[0:600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873bc80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir train_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c1556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_person = [\"{0:012d}.jpg\".format(ids) for ids in imgIds_train]\n",
    "# del_img_train = set(os.listdir(\"coco-2017/train/data\")) - set(train_images_person)\n",
    "for file_name in train_images_person:\n",
    "    src = TRAIN_IMG + \"/\" + file_name\n",
    "    file_path = \"train_small/\" + file_name\n",
    "    if not os.path.exists(file_path):\n",
    "        shutil.copy(src, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb68898",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(os.listdir(\"train_small\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad8e965",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir val_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4bc77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images_person = [\"{0:012d}.jpg\".format(ids) for ids in imgIds_val]\n",
    "# del_img_val = set(os.listdir(VAL_IMG)) - set(val_images_person)\n",
    "i = 1\n",
    "for file_name in val_images_person:\n",
    "    src = VAL_IMG + \"/\" + file_name\n",
    "    file_path = \"val_small/\" + file_name\n",
    "    if not os.path.exists(file_path):\n",
    "        print(i)\n",
    "        shutil.copy(src, file_path)\n",
    "        i+=1\n",
    "\n",
    "len(os.listdir(\"val_small\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1cd172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37392026",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir mask_train_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db4b4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0 \n",
    "\n",
    "for ID in imgIds_train:\n",
    "\n",
    "    file_path = \"./mask_train_2017/COCO_train2017_{0:012d}.jpg\".format(ID)\n",
    "  \n",
    "    sampleImgIds = coco_train.getImgIds(imgIds = [ID])\n",
    "    sampleImgDict = coco_train.loadImgs(sampleImgIds[np.random.randint(0,len(sampleImgIds))])[0]\n",
    "\n",
    "    annIds = coco_train.getAnnIds(imgIds=sampleImgDict['id'], catIds=catIds_train, iscrowd=0)\n",
    "    anns = coco_train.loadAnns(annIds)\n",
    "\n",
    "\n",
    "    mask = coco_train.annToMask(anns[0])\n",
    "    for i in range(len(anns)):\n",
    "        mask = mask | coco_train.annToMask(anns[i])\n",
    "  \n",
    "    mask = Image.fromarray(mask * 255 , mode = \"L\")\n",
    "    mask.save(file_path)\n",
    "    count = count + 1\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be4f6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir mask_val_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea9700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0 \n",
    "for ID in imgIds_val:\n",
    "\n",
    "    file_path = \"./mask_val_2017/COCO_val2017_{0:012d}.jpg\".format(ID)\n",
    "  \n",
    "    sampleImgIds = coco_val.getImgIds(imgIds = [ID])\n",
    "    sampleImgDict = coco_val.loadImgs(sampleImgIds[np.random.randint(0,len(sampleImgIds))])[0]\n",
    "\n",
    "    annIds = coco_val.getAnnIds(imgIds=sampleImgDict['id'], catIds=catIds_val, iscrowd=0)\n",
    "    anns = coco_val.loadAnns(annIds)\n",
    "\n",
    "\n",
    "    mask = coco_val.annToMask(anns[0])\n",
    "    for i in range(len(anns)):\n",
    "        mask = mask | coco_val.annToMask(anns[i])\n",
    "  \n",
    "    mask = Image.fromarray(mask * 255 , mode = \"L\")\n",
    "    mask.save(file_path)\n",
    "  \n",
    "    count = count + 1\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db17f530",
   "metadata": {},
   "source": [
    "# Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e76a11f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-10 07:38:03.082309: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mmkeskar/.local/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/nvidia/lib64:/usr/local/cuda/lib64\n",
      "2022-06-10 07:38:03.082379: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "\n",
    "seed = 0\n",
    "\n",
    "random.seed = seed\n",
    "np.random.seed = seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd5c0092",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGen(tf.keras.utils.Sequence):\n",
    "  \n",
    "    def __init__(self , path_input , path_mask , batch_size = 8 , image_size = 128 , split='train'):\n",
    "    \n",
    "        self.ids = os.listdir(path_input)\n",
    "        self.path_input = path_input\n",
    "        self.path_mask = path_mask\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.on_epoch_end()\n",
    "        self.split = split\n",
    "  \n",
    "    def __load__(self , id_name):\n",
    "    \n",
    "        image_path = os.path.join(self.path_input , id_name)\n",
    "        mask_path = os.path.join(self.path_mask , \"COCO_{}2017_{}\".format(self.split, id_name)) \n",
    "    \n",
    "        image = cv2.imread(image_path , 1) # 1 specifies RGB format\n",
    "        image = cv2.resize(image , (self.image_size , self.image_size)) # resizing before inserting to the network\n",
    "    \n",
    "        mask = cv2.imread(mask_path)\n",
    "        mask = cv2.resize(mask , (self.image_size , self.image_size))\n",
    "        mask = mask[:, :, 0].reshape((self.image_size , self.image_size , 1))\n",
    "      \n",
    "        #normalize image\n",
    "        image = image / 255.0\n",
    "        mask = mask / 255.0\n",
    "    \n",
    "        return image , mask\n",
    "  \n",
    "    def __getitem__(self , index):\n",
    "    \n",
    "        if (index + 1)*self.batch_size > len(self.ids):\n",
    "            self.batch_size = len(self.ids) - index * self.batch_size\n",
    "        \n",
    "        file_batch = self.ids[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "    \n",
    "        images = []\n",
    "        masks = []\n",
    "    \n",
    "        for id_name in file_batch : \n",
    "      \n",
    "            _img , _mask = self.__load__(id_name)\n",
    "            images.append(_img)\n",
    "            masks.append(_mask)\n",
    "    \n",
    "    \n",
    "        images = np.array(images)\n",
    "        masks = np.array(masks)\n",
    "    \n",
    "    \n",
    "        return images , masks\n",
    "  \n",
    "  \n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "  \n",
    "  \n",
    "    def __len__(self):\n",
    "    \n",
    "        return int(np.ceil(len(self.ids) / float(self.batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8d3861",
   "metadata": {},
   "source": [
    "# UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d2ccade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_block(\n",
    "    input_tensor,\n",
    "    no_filters,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(1, 1),\n",
    "    padding=\"same\",\n",
    "    kernel_initializer=\"he_normal\",\n",
    "    max_pool_window=(2, 2),\n",
    "    max_pool_stride=(2, 2)\n",
    "):\n",
    "    conv = Conv2D(\n",
    "        filters=no_filters,\n",
    "        kernel_size=kernel_size,\n",
    "        strides=strides,\n",
    "        activation=None,\n",
    "        padding=padding,\n",
    "        kernel_initializer=kernel_initializer\n",
    "    )(input_tensor)\n",
    "\n",
    "    conv = BatchNormalization(scale=True)(conv)\n",
    "\n",
    "    conv = Activation(\"relu\")(conv)\n",
    "\n",
    "    conv = Conv2D(\n",
    "        filters=no_filters,\n",
    "        kernel_size=kernel_size,\n",
    "        strides=strides,\n",
    "        activation=None,\n",
    "        padding=padding,\n",
    "        kernel_initializer=kernel_initializer\n",
    "    )(conv)\n",
    "\n",
    "    conv = BatchNormalization(scale=True)(conv)\n",
    "\n",
    "    # conv for skip connection\n",
    "    conv = Activation(\"relu\")(conv)\n",
    "\n",
    "    pool = MaxPooling2D(pool_size=max_pool_window, strides=max_pool_stride)(conv)\n",
    "\n",
    "    return conv, pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cff0c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bottle_neck(\n",
    "    input_tensor,\n",
    "    no_filters,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(1, 1),\n",
    "    padding=\"same\",\n",
    "    kernel_initializer=\"he_normal\"\n",
    "):\n",
    "    conv = Conv2D(\n",
    "        filters=no_filters,\n",
    "        kernel_size=kernel_size,\n",
    "        strides=strides,\n",
    "        activation=None,\n",
    "        padding=padding,\n",
    "        kernel_initializer=kernel_initializer\n",
    "    )(input_tensor)\n",
    "\n",
    "    conv = BatchNormalization(scale=True)(conv)\n",
    "\n",
    "    conv = Activation(\"relu\")(conv)\n",
    "\n",
    "    conv = Conv2D(\n",
    "        filters=no_filters,\n",
    "        kernel_size=kernel_size,\n",
    "        strides=strides,\n",
    "        activation=None,\n",
    "        padding=padding,\n",
    "        kernel_initializer=kernel_initializer\n",
    "    )(conv)\n",
    "\n",
    "    conv = BatchNormalization(scale=True)(conv)\n",
    "\n",
    "    conv = Activation(\"relu\")(conv)\n",
    "\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6366e8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_block(    \n",
    "    input_tensor,\n",
    "    no_filters,\n",
    "    skip_connection, \n",
    "    kernel_size=(3, 3),\n",
    "    strides=(1, 1),\n",
    "    upsampling_factor = (2,2),\n",
    "    max_pool_window = (2,2),\n",
    "    padding=\"same\",\n",
    "    kernel_initializer=\"he_normal\"):\n",
    "    \n",
    "    \n",
    "    conv = Conv2D(\n",
    "        filters = no_filters,\n",
    "        kernel_size= max_pool_window,\n",
    "        strides = strides,\n",
    "        activation = None,\n",
    "        padding = padding,\n",
    "        kernel_initializer=kernel_initializer\n",
    "    )(UpSampling2D(size = upsampling_factor)(input_tensor))\n",
    "    \n",
    "    conv = BatchNormalization(scale=True)(conv)\n",
    "\n",
    "    conv = Activation(\"relu\")(conv) \n",
    "    \n",
    "    \n",
    "    conv = concatenate( [skip_connection , conv]  , axis = -1)\n",
    "    \n",
    "    \n",
    "    conv = Conv2D(\n",
    "        filters=no_filters,\n",
    "        kernel_size=kernel_size,\n",
    "        strides=strides,\n",
    "        activation=None,\n",
    "        padding=padding,\n",
    "        kernel_initializer=kernel_initializer\n",
    "    )(conv)\n",
    "\n",
    "    conv = BatchNormalization(scale=True)(conv)\n",
    "\n",
    "    conv = Activation(\"relu\")(conv)\n",
    "\n",
    "    conv = Conv2D(\n",
    "        filters=no_filters,\n",
    "        kernel_size=kernel_size,\n",
    "        strides=strides,\n",
    "        activation=None,\n",
    "        padding=padding,\n",
    "        kernel_initializer=kernel_initializer\n",
    "    )(conv)\n",
    "\n",
    "    conv = BatchNormalization(scale=True)(conv)\n",
    "\n",
    "    conv = Activation(\"relu\")(conv)\n",
    "    \n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c831506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_block(input_tensor,\n",
    "    padding=\"same\",\n",
    "    kernel_initializer=\"he_normal\"\n",
    "):\n",
    "    \n",
    "    conv = Conv2D(\n",
    "        filters=2,\n",
    "        kernel_size=(3,3),\n",
    "        strides=(1,1),\n",
    "        activation=\"relu\",\n",
    "        padding=padding,\n",
    "        kernel_initializer=kernel_initializer\n",
    "    )(input_tensor)\n",
    "    \n",
    "    \n",
    "    conv = Conv2D(\n",
    "        filters=1,\n",
    "        kernel_size=(1,1),\n",
    "        strides=(1,1),\n",
    "        activation=\"relu\",\n",
    "        padding=padding,\n",
    "        kernel_initializer=kernel_initializer\n",
    "    )(conv)\n",
    "    \n",
    "    conv = Conv2D(\n",
    "        filters=1,\n",
    "        kernel_size=(1,1),\n",
    "        strides=(1,1),\n",
    "        activation=\"sigmoid\",\n",
    "        padding=padding,\n",
    "        kernel_initializer=kernel_initializer\n",
    "    )(conv)\n",
    "    \n",
    "    \n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b800a215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNet(input_shape = (128,128,3)):\n",
    "    \n",
    "    filter_size = [32,64,128,256,512]\n",
    "    \n",
    "    inputs = Input(shape = input_shape)\n",
    "    \n",
    "    d1 , p1 = down_block(input_tensor= inputs,\n",
    "                         no_filters=filter_size[0],\n",
    "                         kernel_size = (3,3),\n",
    "                         strides=(1,1),\n",
    "                         padding=\"same\",\n",
    "                         kernel_initializer=\"he_normal\",\n",
    "                         max_pool_window=(2,2),\n",
    "                         max_pool_stride=(2,2))\n",
    "    \n",
    "    \n",
    "    d2 , p2 = down_block(input_tensor= p1,\n",
    "                         no_filters=filter_size[1],\n",
    "                         kernel_size = (3,3),\n",
    "                         strides=(1,1),\n",
    "                         padding=\"same\",\n",
    "                         kernel_initializer=\"he_normal\",\n",
    "                         max_pool_window=(2,2),\n",
    "                         max_pool_stride=(2,2))\n",
    "    \n",
    "    \n",
    "    \n",
    "    d3 , p3 = down_block(input_tensor= p2,\n",
    "                         no_filters=filter_size[2],\n",
    "                         kernel_size = (3,3),\n",
    "                         strides=(1,1),\n",
    "                         padding=\"same\",\n",
    "                         kernel_initializer=\"he_normal\",\n",
    "                         max_pool_window=(2,2),\n",
    "                         max_pool_stride=(2,2))\n",
    "    \n",
    "    \n",
    "    \n",
    "    d4 , p4 = down_block(input_tensor= p3,\n",
    "                         no_filters=filter_size[3],\n",
    "                         kernel_size = (3,3),\n",
    "                         strides=(1,1),\n",
    "                         padding=\"same\",\n",
    "                         kernel_initializer=\"he_normal\",\n",
    "                         max_pool_window=(2,2),\n",
    "                         max_pool_stride=(2,2))\n",
    "    \n",
    "    \n",
    "    b = bottle_neck(input_tensor= p4,\n",
    "                         no_filters=filter_size[4],\n",
    "                         kernel_size = (3,3),\n",
    "                         strides=(1,1),\n",
    "                         padding=\"same\",\n",
    "                         kernel_initializer=\"he_normal\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    u4 = up_block(input_tensor = b,\n",
    "                  no_filters = filter_size[3],\n",
    "                  skip_connection = d4,\n",
    "                  kernel_size=(3, 3),\n",
    "                  strides=(1, 1),\n",
    "                  upsampling_factor = (2,2),\n",
    "                  max_pool_window = (2,2),\n",
    "                  padding=\"same\",\n",
    "                  kernel_initializer=\"he_normal\")\n",
    "    \n",
    "    u3 = up_block(input_tensor = u4,\n",
    "                  no_filters = filter_size[2],\n",
    "                  skip_connection = d3,\n",
    "                  kernel_size=(3, 3),\n",
    "                  strides=(1, 1),\n",
    "                  upsampling_factor = (2,2),\n",
    "                  max_pool_window = (2,2),\n",
    "                  padding=\"same\",\n",
    "                  kernel_initializer=\"he_normal\")\n",
    "    \n",
    "    \n",
    "    u2 = up_block(input_tensor = u3,\n",
    "                  no_filters = filter_size[1],\n",
    "                  skip_connection = d2,\n",
    "                  kernel_size=(3, 3),\n",
    "                  strides=(1, 1),\n",
    "                  upsampling_factor = (2,2),\n",
    "                  max_pool_window = (2,2),\n",
    "                  padding=\"same\",\n",
    "                  kernel_initializer=\"he_normal\")\n",
    "    \n",
    "    \n",
    "    u1 = up_block(input_tensor = u2,\n",
    "                  no_filters = filter_size[0],\n",
    "                  skip_connection = d1,\n",
    "                  kernel_size=(3, 3),\n",
    "                  strides=(1, 1),\n",
    "                  upsampling_factor = (2,2),\n",
    "                  max_pool_window = (2,2),\n",
    "                  padding=\"same\",\n",
    "                  kernel_initializer=\"he_normal\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    output = output_block(input_tensor=u1 , \n",
    "                         padding = \"same\",\n",
    "                         kernel_initializer= \"he_normal\")\n",
    "    \n",
    "    model = Model(inputs = inputs , outputs = output)\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8d03b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 15:16:19.536285: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mmkeskar/.local/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/nvidia/lib64:/usr/local/cuda/lib64\n",
      "2022-06-09 15:16:19.536408: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mmkeskar/.local/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/nvidia/lib64:/usr/local/cuda/lib64\n",
      "2022-06-09 15:16:19.536479: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mmkeskar/.local/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/nvidia/lib64:/usr/local/cuda/lib64\n",
      "2022-06-09 15:16:19.536546: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mmkeskar/.local/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/nvidia/lib64:/usr/local/cuda/lib64\n",
      "2022-06-09 15:16:19.536613: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mmkeskar/.local/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/nvidia/lib64:/usr/local/cuda/lib64\n",
      "2022-06-09 15:16:19.536678: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mmkeskar/.local/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/nvidia/lib64:/usr/local/cuda/lib64\n",
      "2022-06-09 15:16:19.536743: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mmkeskar/.local/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/nvidia/lib64:/usr/local/cuda/lib64\n",
      "2022-06-09 15:16:19.536808: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mmkeskar/.local/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/nvidia/lib64:/usr/local/cuda/lib64\n",
      "2022-06-09 15:16:19.536822: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-06-09 15:16:19.537720: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    model = UNet(input_shape = (128,128,3))\n",
    "    model.compile(optimizer = Adam(learning_rate = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abb3ad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 128 \n",
    "epochs = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "362be950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 72/187 [==========>...................] - ETA: 17:58 - loss: 0.7032 - accuracy: 0.7741"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - 1350s 7s/step - loss: 0.6962 - accuracy: 0.7997 - val_loss: 0.6871 - val_accuracy: 0.8195\n",
      "Epoch 2/10\n",
      "187/187 [==============================] - 838s 4s/step - loss: 0.6841 - accuracy: 0.8172 - val_loss: 0.6811 - val_accuracy: 0.8174\n",
      "Epoch 3/10\n",
      "187/187 [==============================] - 822s 4s/step - loss: 0.6776 - accuracy: 0.8274 - val_loss: 0.6750 - val_accuracy: 0.8178\n",
      "Epoch 4/10\n",
      "187/187 [==============================] - 852s 5s/step - loss: 0.6721 - accuracy: 0.8185 - val_loss: 0.6692 - val_accuracy: 0.8179\n",
      "Epoch 5/10\n",
      "187/187 [==============================] - 830s 4s/step - loss: 0.6664 - accuracy: 0.8185 - val_loss: 0.6636 - val_accuracy: 0.8179\n",
      "Epoch 6/10\n",
      "187/187 [==============================] - 976s 5s/step - loss: 0.6602 - accuracy: 0.8238 - val_loss: 0.6579 - val_accuracy: 0.8179\n",
      "Epoch 7/10\n",
      "187/187 [==============================] - 1074s 6s/step - loss: 0.6547 - accuracy: 0.8220 - val_loss: 0.6524 - val_accuracy: 0.8179\n",
      "Epoch 8/10\n",
      "187/187 [==============================] - 1095s 6s/step - loss: 0.6490 - accuracy: 0.8233 - val_loss: 0.6470 - val_accuracy: 0.8179\n",
      "Epoch 9/10\n",
      "187/187 [==============================] - 1056s 6s/step - loss: 0.6441 - accuracy: 0.8196 - val_loss: 0.6457 - val_accuracy: 0.8064\n",
      "Epoch 10/10\n",
      "187/187 [==============================] - 1051s 6s/step - loss: 0.6372 - accuracy: 0.8271 - val_loss: 0.6798 - val_accuracy: 0.7934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fab706289d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gen = DataGen(path_input = \"train_small\" , path_mask = './mask_train_2017' , batch_size = batch_size , image_size = image_size)\n",
    "val_gen = DataGen(path_input =  \"val_small\", path_mask =  './mask_val_2017', batch_size = batch_size , image_size = image_size, split='val')\n",
    "\n",
    "\n",
    "train_steps =  len(os.listdir(\"train_small\"))/batch_size\n",
    "\n",
    "\n",
    "model.fit(train_gen , validation_data = val_gen , steps_per_epoch = train_steps , epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9be04a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘saved_models’: File exists\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models/unet_model_v3.2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models/unet_model_v3.2/assets\n"
     ]
    }
   ],
   "source": [
    "!mkdir saved_models\n",
    "model.save('saved_models/unet_model_v3.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9369d7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fae7efbe6a0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = val_gen.__getitem__(4)\n",
    "result = model.predict(x)\n",
    "\n",
    "result = result > 0.5\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.imshow(np.reshape(y[0]*255, (image_size, image_size)), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.imshow(np.reshape(result[0]*255, (image_size, image_size)), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76fe82f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-10 07:41:55.922397: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mmkeskar/.local/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/nvidia/lib64:/usr/local/cuda/lib64\n",
      "2022-06-10 07:41:55.922879: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mmkeskar/.local/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/nvidia/lib64:/usr/local/cuda/lib64\n",
      "2022-06-10 07:41:55.923230: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mmkeskar/.local/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/nvidia/lib64:/usr/local/cuda/lib64\n",
      "2022-06-10 07:41:55.923552: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mmkeskar/.local/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/nvidia/lib64:/usr/local/cuda/lib64\n",
      "2022-06-10 07:41:55.923882: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mmkeskar/.local/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/nvidia/lib64:/usr/local/cuda/lib64\n",
      "2022-06-10 07:41:55.924213: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mmkeskar/.local/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/nvidia/lib64:/usr/local/cuda/lib64\n",
      "2022-06-10 07:41:55.924582: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mmkeskar/.local/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/nvidia/lib64:/usr/local/cuda/lib64\n",
      "2022-06-10 07:41:55.924920: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mmkeskar/.local/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/nvidia/lib64:/usr/local/cuda/lib64\n",
      "2022-06-10 07:41:55.924938: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-06-10 07:41:55.925458: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 46s 2s/step - loss: 0.6772 - accuracy: 0.8004\n"
     ]
    }
   ],
   "source": [
    "image_size = 128 \n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "val_gen = DataGen(path_input =  \"val_small\", path_mask =  './mask_val_2017', batch_size = batch_size , image_size = image_size, split='val')\n",
    "\n",
    "model = tf.keras.models.load_model('saved_models/unet_model_v3.2')\n",
    "result = model.evaluate(val_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4755c6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
